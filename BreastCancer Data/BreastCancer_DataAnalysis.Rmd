---
output:
  pdf_document: default
  html_document: default
---
Trishal Varma
Machine Learning



# Step 1
Used str(), head(), and summary() to get information about the data set. Found the percentage between malignant and bening.

Using prop.table to get percentages. 
```{r}
str(BreastCancer)
head(BreastCancer)
summary(BreastCancer$Class)

per <- prop.table(table(BreastCancer$Class))
print(per)
```
a. 65.52% are bening, and 34.47% are malignant. 
b. There are 699 observations and the target column is class.
c. There are a total of 11 variables, and 9 of them can be used as a predictor. 
d. 34.47% are malignant. 
```{r}
names(BreastCancer)
```

# Step 2 
Logistic regression using glm()
```{r}
glm0 <- glm(BreastCancer$Class ~ BreastCancer$Cell.size + BreastCancer$Cell.shape, family = binomial())

summary(glm0)
```
Error Message:: glm.fit: fitted probabilities numerically 0 or 1 occurred

The error code must mean that the model is predicting absolute probabilities of 0 and 1. 

The solution given by researching online suggest to use bayesglm in the arm package. This way we can personalize the coefficients in the regression using glmnet. 

fit < - bayesglm(y ~ x1 + x2, data = d, family ="binomial")
then fit2=glmnet(x,g2,family="binomial") was the given example. 

# Step 3

```{r}
summary(BreastCancer$Cell.size)
summary(BreastCancer$Cell.shape)
BreastCancer$Cell.small <- c(ifelse(BreastCancer$Cell.size ==1,1,0))

BreastCancer$Cell.regular <- c(ifelse(BreastCancer$Cell.shape ==1,1,0))


BreastCancer$Cell.small <- as.factor(BreastCancer$Cell.small)
BreastCancer$Cell.regular <- as.factor(BreastCancer$Cell.regular)

summary(BreastCancer$Cell.small)
summary(BreastCancer$Cell.regular)

```

The distribution of the new columns are 1, and 0. This can be a good idea in determining regression that way graphs can look more readable in terms of data. The data gives us how many 0, and 1 in both columns. 

#Step 4

attached the data so BreastCancer doesn't need to be written everytime. 
```{r}
df <- BreastCancer
attach(df)
par(mfrow = c(1,2))
cdplot(Class ~ Cell.size, xlab = "Cell size", ylab = "Class")
cdplot(Class ~ Cell.shape, xlab = "Cell size", ylab = "Class")
```

The malignant is in the dark, while the benign is in the lighter shade. The bigger it is, the higher chance it is malignant compared to the samller being beningn. 
The cut off does seem justified after looking at the graph. 

# Step 5
```{r}
par(mfrow = c(1,2))
plot(Class ~ Cell.small, xlab = "Cell size", ylab = "Class")
plot(Class ~ Cell.regular, xlab = "Cell shape", ylab = "Class")

smallobs <- sum(Cell.small == 1)
smalllobsMalig <- sum(Cell.small ==1 & Class == "malignant")
smalllobsPct <- (smalllobsMalig / smallobs) * 100
smalllobsPct

largerobs <- sum(Cell.small==0)
largerobsMalig <- sum(Cell.small == 0 & Class =="malignant")
largerobsPct <- (largerobsMalig/largerobs) * 100
largerobsPct

regobs <- sum(Cell.regular==0)
regobsMalig <- sum(Cell.regular==0 & Class == "malignant")
regobsPct <- (regobsMalig/regobs) * 100
regobsPct

nonRegobs <- sum(Cell.regular == 1)
nonRegobsMalig <- sum(Cell.regular == 1 & Class == "malignant")
nonRegobsPct <- (nonRegobsMalig/nonRegobs) * 100
nonRegobsPct

cdplot(Class ~ Cell.small, xlab = "Cell size", ylab = "Class")
cdplot(Class ~ Cell.regular, xlab = "Cell shape", ylab = "Class")

```
Calculated the percentage of small obs. that are malignant. 
Then not-small obs. that are malignant. 
calculated teh regular obs. that are malignant, and then the ones that are non-regular and are malignant. 


a. 1.041667% of small are malignant. 
b. 75.2381% of not small are malignant
c. 69.07514% of regular are malignant
d. 0.5665% of non-regular are malignant 

# Step 6
Dividing the sample into train and test, where the train data is 80% and test 20%. setting the seed to 1234. 
```{r}
data(df)
smp_size <- floor(.80 * nrow(df))

data(df)
set.seed(1234)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

train <- df[train_ind,]
test <- df[-train_ind,] 
```

# Step 7
```{r}
glm1 <- glm(Class~Cell.small + Cell.regular, data = train, family = binomial)
summary(glm1)
```
a. Cell Small and Cell Regular are both better predictor because they both have good P value and are significant. 
b. There is a good from in the Null deviance to the Residual deviance which is good. Null uses just intercept. 
c. We have a AIC value of 261.73. Which is good judging by the fact we have a larger paramenter and is being compared. And noting that AIC values usually are higher if the dataset is larger. 

#Step 8
```{r}
probs <- predict(glm1, newdata = test, type = "response")
pred <- ifelse(probs>0.5,2,1)
acc1 <- mean(pred==as.integer(test$Class))
print(paste("glm1 accuracy = ", acc1))


library(caret)
table(pred, as.integer(test$Class))
confusionMatrix(as.factor(pred), as.factor(as.integer(test$Class)))

```

# Step 9
```{r}
glm1$coefficients[]

probs1 <- mean((exp(as.integer(Cell.small == 1))) / (1 + exp(as.integer(Cell.small == 1))))
probs1

wholeprobs <- (exp(as.integer(Cell.small ==1)))/(1 + exp(as.integer(Cell.small ==1)))
print(wholeprobs)

```
a. The cell small coefficient is -4.6 and cell regular -3.7. Intercept at 1.47. 
b. It means that the indicated event is less likely at this level of predictor than the refrence level. 
c. The estimated probabbility is 62%
d. all over the dataset BreastCancer, it between 50% to 73%. It is the average of the two, so they are close, and gives a good indicated based on the accuracy. 

# Step 10

```{r}
glm_small <- glm(formula = Class ~ Cell.small, family = binomial, data = train)
glm_regular <- glm(formula = Class ~ Cell.regular, family = binomial, data = train)

anova(glm_small, glm_regular, glm1)


AIC(glm_small, glm_regular, glm1)


```
the original dataset made in glm1 data that includes the Cell_small and Cell_regular included is a better indicator compared to them seperated. 

#Step 11
```{r}
nb1 <- naiveBayes(Class ~ Cell.small + Cell.regular, data = train)
nb1
```

a. 65.29% of training data is benign.  
b. There is a 98% chance that a malignant sample is not small. 
c. 83% likelihood that the malignant sample is not regular. 
#Step 12
```{r}
p1 <- predict(nb1, newdata = test, type = "class")
acc <- sum(p1 == test$Class) / nrow(test)
print(paste("Acc: ", acc))
confusionMatrix(test$Class, p1, positive = "malignant")

```

The accurary is the same at 88.57%. The refrence data shows where false positive and false negative cases are. 
